{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Loading Models\n",
    "\n",
    "In this notebook, I'll show you how to save and load models with PyTorch. This is important because you'll often want to load previously trained models to use in making predictions or to continue training on new data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T00:08:13.464864Z",
     "start_time": "2025-05-30T00:08:10.717971Z"
    }
   },
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import helper\n",
    "import fc_model"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T00:08:13.531750Z",
     "start_time": "2025-05-30T00:08:13.466269Z"
    }
   },
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see one of the images."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T00:08:13.695435Z",
     "start_time": "2025-05-30T00:08:13.532613Z"
    }
   },
   "source": [
    "image, label = next(iter(trainloader))\n",
    "helper.imshow(image[0,:]);"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "image/png": {
       "width": 389,
       "height": 389
      }
     },
     "output_type": "display_data"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a network\n",
    "\n",
    "To make things more concise here, I moved the model architecture and training code from the last part to a file called `fc_model`. Importing this, we can easily create a fully-connected network with `fc_model.Network`, and train the network using `fc_model.train`. I'll use this model (once it's trained) to demonstrate how we can save and load models."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T00:08:13.704253Z",
     "start_time": "2025-05-30T00:08:13.696846Z"
    }
   },
   "source": [
    "# Create the network, define the criterion and optimizer\n",
    "\n",
    "model = fc_model.Network(784, 10, [512, 256, 128])\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T00:08:25.697071Z",
     "start_time": "2025-05-30T00:08:13.707479Z"
    }
   },
   "source": [
    "fc_model.train(model, trainloader, testloader, criterion, optimizer, epochs=2)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2..  Training Loss: 1.717..  Test Loss: 0.953..  Test Accuracy: 0.675\n",
      "Epoch: 1/2..  Training Loss: 1.058..  Test Loss: 0.753..  Test Accuracy: 0.708\n",
      "Epoch: 1/2..  Training Loss: 0.872..  Test Loss: 0.683..  Test Accuracy: 0.724\n",
      "Epoch: 1/2..  Training Loss: 0.815..  Test Loss: 0.654..  Test Accuracy: 0.755\n",
      "Epoch: 1/2..  Training Loss: 0.745..  Test Loss: 0.630..  Test Accuracy: 0.757\n",
      "Epoch: 1/2..  Training Loss: 0.699..  Test Loss: 0.600..  Test Accuracy: 0.763\n",
      "Epoch: 1/2..  Training Loss: 0.706..  Test Loss: 0.588..  Test Accuracy: 0.771\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m fc_model\u001B[38;5;241m.\u001B[39mtrain(model, trainloader, testloader, criterion, optimizer, epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-Personal/pythonprojects/deep-learning-v2-pytorch/intro-to-pytorch/fc_model.py:89\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(model, trainloader, testloader, criterion, optimizer, epochs, print_every)\u001B[0m\n\u001B[1;32m     87\u001B[0m \u001B[38;5;66;03m# Turn off gradients for validation, will speed up inference\u001B[39;00m\n\u001B[1;32m     88\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m---> 89\u001B[0m     test_loss, accuracy \u001B[38;5;241m=\u001B[39m validation(model, testloader, criterion)\n\u001B[1;32m     91\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m.. \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(e\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m, epochs),\n\u001B[1;32m     92\u001B[0m       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTraining Loss: \u001B[39m\u001B[38;5;132;01m{:.3f}\u001B[39;00m\u001B[38;5;124m.. \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(running_loss\u001B[38;5;241m/\u001B[39mprint_every),\n\u001B[1;32m     93\u001B[0m       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTest Loss: \u001B[39m\u001B[38;5;132;01m{:.3f}\u001B[39;00m\u001B[38;5;124m.. \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(test_loss\u001B[38;5;241m/\u001B[39m\u001B[38;5;28mlen\u001B[39m(testloader)),\n\u001B[1;32m     94\u001B[0m       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTest Accuracy: \u001B[39m\u001B[38;5;132;01m{:.3f}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(accuracy\u001B[38;5;241m/\u001B[39m\u001B[38;5;28mlen\u001B[39m(testloader)))\n\u001B[1;32m     96\u001B[0m running_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-Personal/pythonprojects/deep-learning-v2-pytorch/intro-to-pytorch/fc_model.py:43\u001B[0m, in \u001B[0;36mvalidation\u001B[0;34m(model, testloader, criterion)\u001B[0m\n\u001B[1;32m     41\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     42\u001B[0m test_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m---> 43\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m images, labels \u001B[38;5;129;01min\u001B[39;00m testloader:\n\u001B[1;32m     45\u001B[0m     images \u001B[38;5;241m=\u001B[39m images\u001B[38;5;241m.\u001B[39mresize_(images\u001B[38;5;241m.\u001B[39msize()[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;241m784\u001B[39m)\n\u001B[1;32m     47\u001B[0m     output \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mforward(images)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:630\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    627\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    628\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    629\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 630\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_data()\n\u001B[1;32m    631\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    632\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    633\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:673\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    671\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    672\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 673\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_fetcher\u001B[38;5;241m.\u001B[39mfetch(index)  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    674\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    675\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     50\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 52\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     54\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/torchvision/datasets/mnist.py:143\u001B[0m, in \u001B[0;36mMNIST.__getitem__\u001B[0;34m(self, index)\u001B[0m\n\u001B[1;32m    139\u001B[0m img, target \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata[index], \u001B[38;5;28mint\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtargets[index])\n\u001B[1;32m    141\u001B[0m \u001B[38;5;66;03m# doing this so that it is consistent with all other datasets\u001B[39;00m\n\u001B[1;32m    142\u001B[0m \u001B[38;5;66;03m# to return a PIL Image\u001B[39;00m\n\u001B[0;32m--> 143\u001B[0m img \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mfromarray(img\u001B[38;5;241m.\u001B[39mnumpy(), mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mL\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    145\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    146\u001B[0m     img \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform(img)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/PIL/Image.py:3154\u001B[0m, in \u001B[0;36mfromarray\u001B[0;34m(obj, mode)\u001B[0m\n\u001B[1;32m   3151\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   3152\u001B[0m         obj \u001B[38;5;241m=\u001B[39m obj\u001B[38;5;241m.\u001B[39mtostring()\n\u001B[0;32m-> 3154\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m frombuffer(mode, size, obj, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraw\u001B[39m\u001B[38;5;124m\"\u001B[39m, rawmode, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/PIL/Image.py:3061\u001B[0m, in \u001B[0;36mfrombuffer\u001B[0;34m(mode, size, data, decoder_name, *args)\u001B[0m\n\u001B[1;32m   3059\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m args[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;129;01min\u001B[39;00m _MAPMODES:\n\u001B[1;32m   3060\u001B[0m     im \u001B[38;5;241m=\u001B[39m new(mode, (\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m))\n\u001B[0;32m-> 3061\u001B[0m     im \u001B[38;5;241m=\u001B[39m im\u001B[38;5;241m.\u001B[39m_new(core\u001B[38;5;241m.\u001B[39mmap_buffer(data, size, decoder_name, \u001B[38;5;241m0\u001B[39m, args))\n\u001B[1;32m   3062\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mP\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   3063\u001B[0m         \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ImagePalette\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading networks\n",
    "\n",
    "As you can imagine, it's impractical to train a network every time you need to use it. Instead, we can save trained networks then load them later to train more or use them for predictions.\n",
    "\n",
    "The parameters for PyTorch networks are stored in a model's `state_dict`. We can see the state dict contains the weight and bias matrices for each of our layers."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Our model: \\n\\n\", model, '\\n')\n",
    "print(\"The state dict keys: \\n\\n\", model.state_dict().keys())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest thing to do is simply save the state dict with `torch.save`. For example, we can save it to a file `'checkpoint.pth'`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "torch.save(model.state_dict(), 'checkpoint.pth')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can load the state dict with `torch.load`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "state_dict = torch.load('checkpoint.pth')\n",
    "print(state_dict.keys())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to load the state dict in to the network, you do `model.load_state_dict(state_dict)`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model.load_state_dict(state_dict)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems pretty straightforward, but as usual it's a bit more complicated. Loading the state dict works only if the model architecture is exactly the same as the checkpoint architecture. If I create a model with a different architecture, this fails."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Try this\n",
    "model = fc_model.Network(784, 10, [400, 200, 100])\n",
    "# This will throw an error because the tensor sizes are wrong!\n",
    "model.load_state_dict(state_dict)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means we need to rebuild the model exactly as it was when trained. Information about the model architecture needs to be saved in the checkpoint, along with the state dict. To do this, you build a dictionary with all the information you need to compeletely rebuild the model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "checkpoint = {'input_size': 784,\n",
    "              'output_size': 10,\n",
    "              'hidden_layers': [each.out_features for each in model.hidden_layers],\n",
    "              'state_dict': model.state_dict()}\n",
    "\n",
    "torch.save(checkpoint, 'checkpoint.pth')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the checkpoint has all the necessary information to rebuild the trained model. You can easily make that a function if you want. Similarly, we can write a function to load checkpoints. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def load_checkpoint(filepath):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model = fc_model.Network(checkpoint['input_size'],\n",
    "                             checkpoint['output_size'],\n",
    "                             checkpoint['hidden_layers'])\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    \n",
    "    return model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model = load_checkpoint('checkpoint.pth')\n",
    "print(model)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
